## 数据获取

我们通过爬虫对新浪微博、江苏电视台荔枝新闻和新浪新闻进行了数据的爬取。这其中，对新浪微博的爬取采用的是通过移动端网页版API（m.weibo.cn）爬取的方式，而对荔枝新闻和新浪新闻的爬取则采用对html源码进行解析的方式。详情可以在我们的开源仓库中查看。由于在开发新浪微博爬虫的过程中和郝晨杰同学有密切的合作，故我们在征得对方同意后使用了郝晨杰同学获取的数据。我们获取的数据在经过处理后均为json格式的数据，读取和写入十分方便。

## 数据分析

为了从更细的粒度上考察随着新冠疫情变化而不断变化的社会心态，我们将研究的基本单位定为天。在不同阶段的文件夹下，每一天分割后的原始数据和处理后的数据结果都在阶段文件夹下的对应日期文件夹内，而该阶段整体上的基础信息则写在该阶段文件下。由于不同阶段本身的特点，不同日期内拥有的数据种类可能有所不同，但是总体的命名格式是统一的。例如，在潜在的与疫情相关的关键词的选取上，在第0阶段和第1阶段，荔枝新闻没有对应文章的，关键词生成的源文本就会用新浪新闻的标题做补充。详细情况可以看`data`文件夹下的`README`文件。

我们首先提取了不同阶段的疫情关键词。这一步操作对每一个阶段而言都是分别进行的。首先，我们从以“肺炎”为关键词检索的荔枝新闻正文（部分采用了新浪新闻标题）中分词、去除停用词后对每一天生成了前36个关键词（生成的结果已经归一化，最大权值的词的权值为1），并将数据乘以100后绘制词云图（对所有值下取整后）并存储起来。在这些词云图中，我们可以鲜明地看到每一天的疫情相关新闻的重点的演变。在此基础上，我们将这些结果加和汇总，即相同的词语的权值相加，得到一份较大的关键词表，而后再对这其中的每个词语的权值取10的对数并归一化，得到一份原始疫情相关关键词表。我们发现，这样的设计能有效防止因某个关键词的权值过高（如“新冠”、“肺炎”而导致一出现该词对应文本的最终权重就可能过高），也能避免对所有关键词直接采用TF-IDF或者TextRank算法时可能会漏掉一些仅在特定日子中出现但却和疫情演变进程密切相关的关键词。显然，上面所述步骤里并没有人工的介入，关键词中难免混入一些杂质。因此我们还人工地对这些关键词进行了筛选。

而后，我们在每一阶段的疫情相关关键词的基础上，筛选出与疫情相关度较高的微博作为后面进一步分析的数据源。在这一步骤中，我们先对微博正文进行分词，然后在分词后的结果中一一检索是否有关键词表中的词，如果有，就给该微博的疫情相关度加上该词的权重乘以该词的出现次数。在这一步中，对每条微博的评论进行精简，仅保留前五分之一的评论。在得到所有微博的疫情相关度指标后，将其从小到大排序并进行函数拟合，拟合函数如下：
$$
f ( x ) = a * b^{ x - d } +c 
$$
在拟合后，先计算 $MSE$、$RMSE$ 和 $R^2$ 以检验拟合的效果，并绘制对应图像，从图像中观察相关度的分布。与此同时，图像中还绘制了微博相关度由小到大排序后对应的热度（从三个指标来看：点赞数、评论数和转发数）的散点图以探究疫情相关度高低与热度高低之间的关系。

在此之后，我们利用拟合的曲线取某个上百分位点作为阈值(默认选取函数图像上的“拐点”，并加以人工复核与调整），疫情相关度高于该阈值的微博视作为疫情相关重点微博，并将所有疫情相关重点微博存储到对应文件中。与此同时，计算并绘制每日的疫情相关微博占比，以反映当日当日央媒对疫情相关新闻的关注度与当日疫情相关消息的重要程度。

在这些步骤进行后，我们在一定程度上掌握了不同阶段下新冠疫情相关信息的分布、热度及重要程度，为下面都分析展开了基础。